{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eabf088",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a164957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc15f8b7",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e3af0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"angry\", \n",
    "    \"disgust\", \n",
    "    \"fear\", \n",
    "    \"happy\", \n",
    "    \"sad\", \n",
    "    \"surprise\", \n",
    "    \"neutral\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85ac1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # dataset\n",
    "    TRAIN_DS_PATH = './dataset/train.csv'\n",
    "    VAL_DS_PATH = './dataset/val.csv'\n",
    "    TEST_DS_PATH = './dataset/finaltest.csv'\n",
    "    \n",
    "    # images dir\n",
    "    TRAIN_IMG_DIR = './dataset/train/'\n",
    "    VAL_IMG_DIR  = './dataset/val/'\n",
    "    TEST_IMG_DIR  = './dataset/finaltest/'\n",
    "    \n",
    "    # training hyperparams\n",
    "    EPOCHS = 50\n",
    "    LR = 1e-2\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0\n",
    "    SHUFFLE = True\n",
    "    \n",
    "    # saved model path\n",
    "    MODEL_DIR = './model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a43450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1913c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard writer\n",
    "writer = SummaryWriter(\"runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd927d2d",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e5d1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/omarsayed7/Deep-Emotion/blob/master/data_loaders.py\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, datatype, transform):\n",
    "        '''\n",
    "        Pytorch Dataset class\n",
    "        params:-\n",
    "                 csv_file : the path of the csv file    (train, validation, test)\n",
    "                 img_dir  : the directory of the images (train, validation, test)\n",
    "                 datatype : data type for the dataset   (train, val, test)\n",
    "                 transform: pytorch transformation over the data\n",
    "        return :-\n",
    "                 image, labels\n",
    "        '''\n",
    "        self.csv_file = pd.read_csv(csv_file)\n",
    "        self.labels = self.csv_file['emotion']\n",
    "        self.img_dir = img_dir\n",
    "        self.datatype = datatype\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img = Image.open(self.img_dir + self.datatype + str(idx) + '.jpg').convert('RGB')\n",
    "        labels = np.array(self.labels[idx])\n",
    "        labels = torch.from_numpy(labels).long()\n",
    "\n",
    "        if self.transform :\n",
    "            img = self.transform(img)\n",
    "        return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4517638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranformations\n",
    "transformation= transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "#     transforms.Grayscale(3), # no need this since you .convert(\"RGB\") at __getitem__\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5192792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset = EmotionDataset(\n",
    "    csv_file=cfg.TRAIN_DS_PATH, \n",
    "    img_dir=cfg.TRAIN_IMG_DIR,\n",
    "    datatype='train',\n",
    "    transform=transformation\n",
    ")\n",
    "\n",
    "validation_dataset = EmotionDataset(\n",
    "    csv_file=cfg.VAL_DS_PATH, \n",
    "    img_dir=cfg.VAL_IMG_DIR,\n",
    "    datatype='val',\n",
    "    transform = transformation\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = EmotionDataset(\n",
    "    csv_file=cfg.TEST_DS_PATH, \n",
    "    img_dir=cfg.TEST_IMG_DIR,\n",
    "    datatype='finaltest',\n",
    "    transform = transformation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ff7dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=cfg.BATCH_SIZE, \n",
    "    shuffle =cfg.SHUFFLE, \n",
    "    num_workers=cfg.NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=cfg.BATCH_SIZE, \n",
    "    shuffle =cfg.SHUFFLE, \n",
    "    num_workers=cfg.NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=cfg.BATCH_SIZE, \n",
    "    shuffle =cfg.SHUFFLE, \n",
    "    num_workers=cfg.NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5df5358",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d680aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, train_loader, criterion, optimizer, device, steps_print_loss=0, writer=None):\n",
    "    \"\"\"Train the model for 1 epoch\n",
    "    Args:\n",
    "        epoch: Current training epoch\n",
    "        model: nn.Module\n",
    "        train_loader: train DataLoader\n",
    "        criterion: callable loss function\n",
    "        optimizer: pytorch optimizer\n",
    "        device: torch.device\n",
    "        steps_print_loss: loss will print out in every specified steps\n",
    "        writer: Tensorboard SummaryWriter\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Float, Float]\n",
    "        average train loss and average train accuracy for current epoch\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    train_corrects = []\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over data.\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # prediction\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # calculate loss\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        train_losses.append(loss.item())\n",
    "        train_corrects.append(torch.sum(preds == labels.data).item())\n",
    "        \n",
    "        if steps_print_loss and (batch_idx % steps_print_loss == 0):\n",
    "            print(f\"loss: {loss.item():>7f}  [{batch_idx:>5d}/{len(train_loader):>5d}]\")\n",
    "        \n",
    "        if writer:\n",
    "            writer.add_scalar(\n",
    "                \"train_loss\", \n",
    "                sum(train_losses)/len(train_losses),\n",
    "                global_steps=epoch*len(train_loader)*batch_idx\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"train_accuracy\",\n",
    "                sum(train_corrects)/len(train_loader.dataset),\n",
    "                global_steps=epoch*len(train_loader)*batch_idx\n",
    "            )\n",
    "        \n",
    "    ave_train_loss = sum(train_losses)/len(train_losses)\n",
    "    ave_train_accuracy = sum(train_corrects)/len(train_loader.dataset)      \n",
    "\n",
    "    return ave_train_loss, ave_train_accuracy\n",
    "\n",
    "\n",
    "def val_epoch(epoch, model, val_loader, criterion, device, writer=None):\n",
    "    \"\"\"Validate the model for 1 epoch\n",
    "    Args:\n",
    "        epoch: Current validation epoch\n",
    "        model: nn.Module\n",
    "        val_loader: val DataLoader\n",
    "        criterion: callable loss function\n",
    "        device: torch.device\n",
    "        writer: Tensorboard SummaryWriter\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Float, Float]\n",
    "        average val loss and average val accuracy for current epoch\n",
    "    \"\"\"\n",
    "\n",
    "    val_losses = []\n",
    "    val_corrects = []\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over data\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # prediction\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # calculate loss\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # statistics\n",
    "            val_losses.append(loss.item())\n",
    "            val_corrects.append(torch.sum(preds == labels.data).item())\n",
    "            \n",
    "            if writer:\n",
    "                writer.add_scalar(\n",
    "                    \"val_loss\", \n",
    "                    sum(val_losses)/len(val_losses),\n",
    "                    global_steps=epoch*len(val_loader)*batch_idx\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"val_accuracy\",\n",
    "                    sum(val_corrects)/len(val_loader.dataset),\n",
    "                    global_steps=epoch*len(val_loader)*batch_idx\n",
    "                )\n",
    "            \n",
    "        ave_val_loss = sum(val_losses)/len(val_losses)\n",
    "        ave_val_accuracy = sum(val_corrects)/len(val_loader.dataset)\n",
    "\n",
    "    return ave_val_loss, ave_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97be22",
   "metadata": {},
   "source": [
    "# Training and Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19b36628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "model = models.mobilenet.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "888b5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_features, len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "efe3cfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# transfer to cuda device if any\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c76f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, optimizer and scheduler \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=cfg.LR)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a3a93309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.140212  [    0/ 3589]\n",
      "loss: 2.059390  [  500/ 3589]\n",
      "loss: 2.356801  [ 1000/ 3589]\n",
      "loss: 9.660825  [ 1500/ 3589]\n",
      "loss: 2.275562  [ 2000/ 3589]\n",
      "loss: 3.315902  [ 2500/ 3589]\n",
      "loss: 3.490290  [ 3000/ 3589]\n",
      "loss: 2.930248  [ 3500/ 3589]\n",
      "Average TrainLoss: 3.6870 \tAverage TrainAcc: 0.2584\n",
      "Evaluation:\n",
      "Average ValLoss: 3.3211 \tAverage ValAcc: 0.2856\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 4.368715  [    0/ 3589]\n",
      "loss: 5.648328  [  500/ 3589]\n",
      "loss: 3.797549  [ 1000/ 3589]\n",
      "loss: 4.148750  [ 1500/ 3589]\n",
      "loss: 1.285423  [ 2000/ 3589]\n",
      "loss: 3.271886  [ 2500/ 3589]\n",
      "loss: 3.513383  [ 3000/ 3589]\n",
      "loss: 3.547631  [ 3500/ 3589]\n",
      "Average TrainLoss: 3.9700 \tAverage TrainAcc: 0.2653\n",
      "Evaluation:\n",
      "Average ValLoss: 2.5226 \tAverage ValAcc: 0.3346\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.579693  [    0/ 3589]\n",
      "loss: 6.319479  [  500/ 3589]\n",
      "loss: 3.984198  [ 1000/ 3589]\n",
      "loss: 1.539312  [ 1500/ 3589]\n",
      "loss: 3.414398  [ 2000/ 3589]\n",
      "loss: 6.989782  [ 2500/ 3589]\n",
      "loss: 6.338362  [ 3000/ 3589]\n",
      "loss: 3.232034  [ 3500/ 3589]\n",
      "Average TrainLoss: 3.9686 \tAverage TrainAcc: 0.2684\n",
      "Evaluation:\n",
      "Average ValLoss: 4.7264 \tAverage ValAcc: 0.2563\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 5.424212  [    0/ 3589]\n",
      "loss: 2.486997  [  500/ 3589]\n",
      "loss: 3.167638  [ 1000/ 3589]\n",
      "loss: 1.686496  [ 1500/ 3589]\n",
      "loss: 4.993395  [ 2000/ 3589]\n",
      "loss: 2.924614  [ 2500/ 3589]\n",
      "loss: 4.076925  [ 3000/ 3589]\n",
      "loss: 2.881586  [ 3500/ 3589]\n",
      "Average TrainLoss: 3.8739 \tAverage TrainAcc: 0.2674\n",
      "Evaluation:\n",
      "Average ValLoss: 4.1380 \tAverage ValAcc: 0.3266\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.808155  [    0/ 3589]\n",
      "loss: 1.920478  [  500/ 3589]\n",
      "loss: 5.329133  [ 1000/ 3589]\n",
      "loss: 2.057617  [ 1500/ 3589]\n",
      "loss: 4.009713  [ 2000/ 3589]\n",
      "loss: 4.966100  [ 2500/ 3589]\n",
      "loss: 4.197045  [ 3000/ 3589]\n",
      "loss: 2.949345  [ 3500/ 3589]\n",
      "Average TrainLoss: 4.0109 \tAverage TrainAcc: 0.2697\n",
      "Evaluation:\n",
      "Average ValLoss: 2.6008 \tAverage ValAcc: 0.3533\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 4.478968  [    0/ 3589]\n",
      "loss: 7.220558  [  500/ 3589]\n",
      "loss: 4.677662  [ 1000/ 3589]\n",
      "loss: 2.745539  [ 1500/ 3589]\n",
      "loss: 6.250101  [ 2000/ 3589]\n",
      "loss: 1.507144  [ 2500/ 3589]\n",
      "loss: 3.123695  [ 3000/ 3589]\n",
      "loss: 4.297137  [ 3500/ 3589]\n",
      "Average TrainLoss: 3.9541 \tAverage TrainAcc: 0.2672\n",
      "Evaluation:\n",
      "Average ValLoss: 3.8774 \tAverage ValAcc: 0.3299\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 4.351251  [    0/ 3589]\n",
      "loss: 1.557784  [  500/ 3589]\n",
      "loss: 3.270426  [ 1000/ 3589]\n",
      "loss: 4.372365  [ 1500/ 3589]\n",
      "loss: 3.713729  [ 2000/ 3589]\n",
      "loss: 4.679249  [ 2500/ 3589]\n",
      "loss: 5.436192  [ 3000/ 3589]\n",
      "loss: 4.326982  [ 3500/ 3589]\n",
      "Average TrainLoss: 3.9184 \tAverage TrainAcc: 0.2700\n",
      "Evaluation:\n",
      "Average ValLoss: 2.6549 \tAverage ValAcc: 0.3433\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.327107  [    0/ 3589]\n",
      "loss: 1.867272  [  500/ 3589]\n",
      "loss: 3.798499  [ 1000/ 3589]\n",
      "loss: 4.924834  [ 1500/ 3589]\n",
      "loss: 3.434375  [ 2000/ 3589]\n",
      "loss: 3.593422  [ 2500/ 3589]\n",
      "loss: 5.348771  [ 3000/ 3589]\n",
      "loss: 2.431265  [ 3500/ 3589]\n",
      "Average TrainLoss: 4.0126 \tAverage TrainAcc: 0.2717\n",
      "Evaluation:\n",
      "Average ValLoss: 2.9965 \tAverage ValAcc: 0.2878\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.699262  [    0/ 3589]\n",
      "loss: 4.478236  [  500/ 3589]\n",
      "loss: 3.384045  [ 1000/ 3589]\n",
      "loss: 4.554862  [ 1500/ 3589]\n",
      "loss: 2.547971  [ 2000/ 3589]\n",
      "loss: 9.703259  [ 2500/ 3589]\n",
      "loss: 4.663969  [ 3000/ 3589]\n",
      "loss: 5.804157  [ 3500/ 3589]\n",
      "Average TrainLoss: 3.9205 \tAverage TrainAcc: 0.2676\n",
      "Evaluation:\n",
      "Average ValLoss: 4.1630 \tAverage ValAcc: 0.3015\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 4.404164  [    0/ 3589]\n",
      "loss: 3.665134  [  500/ 3589]\n",
      "loss: 4.663191  [ 1000/ 3589]\n",
      "loss: 8.363679  [ 1500/ 3589]\n",
      "loss: 2.194124  [ 2000/ 3589]\n",
      "loss: 4.542486  [ 2500/ 3589]\n",
      "loss: 6.482517  [ 3000/ 3589]\n",
      "loss: 5.199369  [ 3500/ 3589]\n",
      "Average TrainLoss: 3.9746 \tAverage TrainAcc: 0.2703\n",
      "Evaluation:\n",
      "Average ValLoss: 2.6253 \tAverage ValAcc: 0.2887\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 4.465837  [    0/ 3589]\n",
      "loss: 2.385306  [  500/ 3589]\n",
      "loss: 1.585526  [ 1000/ 3589]\n",
      "loss: 4.040329  [ 1500/ 3589]\n",
      "loss: 2.519892  [ 2000/ 3589]\n",
      "loss: 6.363555  [ 2500/ 3589]\n",
      "loss: 2.604355  [ 3000/ 3589]\n",
      "loss: 4.218763  [ 3500/ 3589]\n",
      "Average TrainLoss: 3.9986 \tAverage TrainAcc: 0.2704\n",
      "Evaluation:\n",
      "Average ValLoss: 3.1190 \tAverage ValAcc: 0.3040\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 5.278955  [    0/ 3589]\n",
      "loss: 4.153754  [  500/ 3589]\n",
      "loss: 2.445649  [ 1000/ 3589]\n",
      "loss: 2.923440  [ 1500/ 3589]\n",
      "loss: 4.689580  [ 2000/ 3589]\n",
      "loss: 6.299421  [ 2500/ 3589]\n",
      "loss: 3.496863  [ 3000/ 3589]\n",
      "loss: 4.184505  [ 3500/ 3589]\n",
      "Average TrainLoss: 3.9888 \tAverage TrainAcc: 0.2697\n",
      "Evaluation:\n",
      "Average ValLoss: 3.3321 \tAverage ValAcc: 0.3210\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.976904  [    0/ 3589]\n",
      "loss: 1.850351  [  500/ 3589]\n",
      "loss: 4.198851  [ 1000/ 3589]\n",
      "loss: 5.373555  [ 1500/ 3589]\n",
      "loss: 1.567453  [ 2000/ 3589]\n",
      "loss: 2.130676  [ 2500/ 3589]\n",
      "loss: 4.772889  [ 3000/ 3589]\n",
      "loss: 3.289534  [ 3500/ 3589]\n",
      "Average TrainLoss: 4.0146 \tAverage TrainAcc: 0.2628\n",
      "Evaluation:\n",
      "Average ValLoss: 3.9698 \tAverage ValAcc: 0.2678\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 5.037828  [    0/ 3589]\n",
      "loss: 2.464392  [  500/ 3589]\n",
      "loss: 3.446841  [ 1000/ 3589]\n",
      "loss: 2.989739  [ 1500/ 3589]\n",
      "loss: 1.970933  [ 2000/ 3589]\n",
      "loss: 2.045505  [ 2500/ 3589]\n",
      "loss: 1.940176  [ 3000/ 3589]\n",
      "loss: 2.065560  [ 3500/ 3589]\n",
      "Average TrainLoss: 2.3975 \tAverage TrainAcc: 0.3111\n",
      "Evaluation:\n",
      "Average ValLoss: 1.7645 \tAverage ValAcc: 0.3714\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.353307  [    0/ 3589]\n",
      "loss: 1.917372  [  500/ 3589]\n",
      "loss: 2.527443  [ 1000/ 3589]\n",
      "loss: 1.520231  [ 1500/ 3589]\n",
      "loss: 0.929334  [ 2000/ 3589]\n",
      "loss: 1.513227  [ 2500/ 3589]\n",
      "loss: 2.459360  [ 3000/ 3589]\n",
      "loss: 1.861539  [ 3500/ 3589]\n",
      "Average TrainLoss: 2.0451 \tAverage TrainAcc: 0.3167\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6174 \tAverage ValAcc: 0.4043\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.112267  [    0/ 3589]\n",
      "loss: 1.015725  [  500/ 3589]\n",
      "loss: 2.351817  [ 1000/ 3589]\n",
      "loss: 2.102896  [ 1500/ 3589]\n",
      "loss: 2.035069  [ 2000/ 3589]\n",
      "loss: 2.145736  [ 2500/ 3589]\n",
      "loss: 1.159142  [ 3000/ 3589]\n",
      "loss: 0.962327  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.8768 \tAverage TrainAcc: 0.3259\n",
      "Evaluation:\n",
      "Average ValLoss: 1.7484 \tAverage ValAcc: 0.3486\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.616157  [    0/ 3589]\n",
      "loss: 1.680263  [  500/ 3589]\n",
      "loss: 1.798198  [ 1000/ 3589]\n",
      "loss: 2.524428  [ 1500/ 3589]\n",
      "loss: 2.163161  [ 2000/ 3589]\n",
      "loss: 1.528087  [ 2500/ 3589]\n",
      "loss: 2.168808  [ 3000/ 3589]\n",
      "loss: 2.135029  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.8139 \tAverage TrainAcc: 0.3262\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5937 \tAverage ValAcc: 0.3968\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.520640  [    0/ 3589]\n",
      "loss: 1.206323  [  500/ 3589]\n",
      "loss: 2.892213  [ 1000/ 3589]\n",
      "loss: 1.045710  [ 1500/ 3589]\n",
      "loss: 1.603242  [ 2000/ 3589]\n",
      "loss: 1.921008  [ 2500/ 3589]\n",
      "loss: 1.500516  [ 3000/ 3589]\n",
      "loss: 1.543084  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7772 \tAverage TrainAcc: 0.3332\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5971 \tAverage ValAcc: 0.3873\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.766872  [    0/ 3589]\n",
      "loss: 2.149664  [  500/ 3589]\n",
      "loss: 1.169902  [ 1000/ 3589]\n",
      "loss: 2.235720  [ 1500/ 3589]\n",
      "loss: 2.378092  [ 2000/ 3589]\n",
      "loss: 1.551165  [ 2500/ 3589]\n",
      "loss: 1.134132  [ 3000/ 3589]\n",
      "loss: 1.634149  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7673 \tAverage TrainAcc: 0.3336\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6356 \tAverage ValAcc: 0.3639\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.506153  [    0/ 3589]\n",
      "loss: 2.364539  [  500/ 3589]\n",
      "loss: 1.569610  [ 1000/ 3589]\n",
      "loss: 1.902600  [ 1500/ 3589]\n",
      "loss: 2.040821  [ 2000/ 3589]\n",
      "loss: 2.410606  [ 2500/ 3589]\n",
      "loss: 2.237992  [ 3000/ 3589]\n",
      "loss: 1.931938  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7597 \tAverage TrainAcc: 0.3345\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5428 \tAverage ValAcc: 0.4035\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.145669  [    0/ 3589]\n",
      "loss: 1.284370  [  500/ 3589]\n",
      "loss: 1.083313  [ 1000/ 3589]\n",
      "loss: 1.409764  [ 1500/ 3589]\n",
      "loss: 1.789906  [ 2000/ 3589]\n",
      "loss: 1.622682  [ 2500/ 3589]\n",
      "loss: 1.661300  [ 3000/ 3589]\n",
      "loss: 2.047638  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7515 \tAverage TrainAcc: 0.3350\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ValLoss: 1.5408 \tAverage ValAcc: 0.4062\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.395439  [    0/ 3589]\n",
      "loss: 2.170592  [  500/ 3589]\n",
      "loss: 1.850816  [ 1000/ 3589]\n",
      "loss: 1.356704  [ 1500/ 3589]\n",
      "loss: 2.013103  [ 2000/ 3589]\n",
      "loss: 1.604768  [ 2500/ 3589]\n",
      "loss: 1.258372  [ 3000/ 3589]\n",
      "loss: 2.230244  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7378 \tAverage TrainAcc: 0.3402\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5347 \tAverage ValAcc: 0.4046\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.086190  [    0/ 3589]\n",
      "loss: 1.687603  [  500/ 3589]\n",
      "loss: 1.498874  [ 1000/ 3589]\n",
      "loss: 1.533374  [ 1500/ 3589]\n",
      "loss: 2.006775  [ 2000/ 3589]\n",
      "loss: 1.365455  [ 2500/ 3589]\n",
      "loss: 1.793460  [ 3000/ 3589]\n",
      "loss: 1.996094  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7446 \tAverage TrainAcc: 0.3332\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6488 \tAverage ValAcc: 0.3564\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.042656  [    0/ 3589]\n",
      "loss: 1.326475  [  500/ 3589]\n",
      "loss: 2.111315  [ 1000/ 3589]\n",
      "loss: 2.155059  [ 1500/ 3589]\n",
      "loss: 1.553187  [ 2000/ 3589]\n",
      "loss: 1.913752  [ 2500/ 3589]\n",
      "loss: 1.244231  [ 3000/ 3589]\n",
      "loss: 1.878783  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7400 \tAverage TrainAcc: 0.3356\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6587 \tAverage ValAcc: 0.3578\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.591395  [    0/ 3589]\n",
      "loss: 1.609294  [  500/ 3589]\n",
      "loss: 1.406108  [ 1000/ 3589]\n",
      "loss: 1.770317  [ 1500/ 3589]\n",
      "loss: 1.488628  [ 2000/ 3589]\n",
      "loss: 1.454930  [ 2500/ 3589]\n",
      "loss: 1.622166  [ 3000/ 3589]\n",
      "loss: 1.938580  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7455 \tAverage TrainAcc: 0.3374\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5341 \tAverage ValAcc: 0.4040\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.462383  [    0/ 3589]\n",
      "loss: 1.728094  [  500/ 3589]\n",
      "loss: 1.703583  [ 1000/ 3589]\n",
      "loss: 1.543194  [ 1500/ 3589]\n",
      "loss: 1.517702  [ 2000/ 3589]\n",
      "loss: 1.356445  [ 2500/ 3589]\n",
      "loss: 1.807006  [ 3000/ 3589]\n",
      "loss: 1.668903  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7332 \tAverage TrainAcc: 0.3410\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5855 \tAverage ValAcc: 0.3926\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.995954  [    0/ 3589]\n",
      "loss: 2.431480  [  500/ 3589]\n",
      "loss: 1.019544  [ 1000/ 3589]\n",
      "loss: 1.857826  [ 1500/ 3589]\n",
      "loss: 1.979622  [ 2000/ 3589]\n",
      "loss: 1.684474  [ 2500/ 3589]\n",
      "loss: 2.029061  [ 3000/ 3589]\n",
      "loss: 1.801249  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7387 \tAverage TrainAcc: 0.3385\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6270 \tAverage ValAcc: 0.3692\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.054143  [    0/ 3589]\n",
      "loss: 2.128859  [  500/ 3589]\n",
      "loss: 1.915770  [ 1000/ 3589]\n",
      "loss: 1.766568  [ 1500/ 3589]\n",
      "loss: 1.661186  [ 2000/ 3589]\n",
      "loss: 1.524379  [ 2500/ 3589]\n",
      "loss: 1.851828  [ 3000/ 3589]\n",
      "loss: 1.752945  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7366 \tAverage TrainAcc: 0.3388\n",
      "Evaluation:\n",
      "Average ValLoss: 1.7859 \tAverage ValAcc: 0.3633\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.392760  [    0/ 3589]\n",
      "loss: 1.435691  [  500/ 3589]\n",
      "loss: 1.453282  [ 1000/ 3589]\n",
      "loss: 1.809755  [ 1500/ 3589]\n",
      "loss: 1.333560  [ 2000/ 3589]\n",
      "loss: 1.545719  [ 2500/ 3589]\n",
      "loss: 1.021682  [ 3000/ 3589]\n",
      "loss: 1.780612  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7368 \tAverage TrainAcc: 0.3352\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6710 \tAverage ValAcc: 0.3631\n",
      "\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.218625  [    0/ 3589]\n",
      "loss: 1.701584  [  500/ 3589]\n",
      "loss: 1.865880  [ 1000/ 3589]\n",
      "loss: 2.344922  [ 1500/ 3589]\n",
      "loss: 2.411024  [ 2000/ 3589]\n",
      "loss: 2.344086  [ 2500/ 3589]\n",
      "loss: 1.507183  [ 3000/ 3589]\n",
      "loss: 1.719331  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7351 \tAverage TrainAcc: 0.3399\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5864 \tAverage ValAcc: 0.3876\n",
      "\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.074393  [    0/ 3589]\n",
      "loss: 1.932308  [  500/ 3589]\n",
      "loss: 1.807935  [ 1000/ 3589]\n",
      "loss: 2.278069  [ 1500/ 3589]\n",
      "loss: 1.262973  [ 2000/ 3589]\n",
      "loss: 1.711307  [ 2500/ 3589]\n",
      "loss: 1.951457  [ 3000/ 3589]\n",
      "loss: 1.252833  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7353 \tAverage TrainAcc: 0.3383\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6183 \tAverage ValAcc: 0.3742\n",
      "\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.062488  [    0/ 3589]\n",
      "loss: 1.609925  [  500/ 3589]\n",
      "loss: 2.001439  [ 1000/ 3589]\n",
      "loss: 2.378935  [ 1500/ 3589]\n",
      "loss: 1.618739  [ 2000/ 3589]\n",
      "loss: 1.659097  [ 2500/ 3589]\n",
      "loss: 2.073653  [ 3000/ 3589]\n",
      "loss: 2.130551  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7345 \tAverage TrainAcc: 0.3365\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6224 \tAverage ValAcc: 0.3840\n",
      "\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.096688  [    0/ 3589]\n",
      "loss: 2.593363  [  500/ 3589]\n",
      "loss: 1.890658  [ 1000/ 3589]\n",
      "loss: 1.265965  [ 1500/ 3589]\n",
      "loss: 1.734234  [ 2000/ 3589]\n",
      "loss: 1.076675  [ 2500/ 3589]\n",
      "loss: 1.165624  [ 3000/ 3589]\n",
      "loss: 1.853680  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7403 \tAverage TrainAcc: 0.3396\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6063 \tAverage ValAcc: 0.3798\n",
      "\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.926066  [    0/ 3589]\n",
      "loss: 1.718588  [  500/ 3589]\n",
      "loss: 1.929847  [ 1000/ 3589]\n",
      "loss: 2.346808  [ 1500/ 3589]\n",
      "loss: 1.385443  [ 2000/ 3589]\n",
      "loss: 2.065623  [ 2500/ 3589]\n",
      "loss: 1.358596  [ 3000/ 3589]\n",
      "loss: 1.913431  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7448 \tAverage TrainAcc: 0.3297\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5313 \tAverage ValAcc: 0.4051\n",
      "\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.577537  [    0/ 3589]\n",
      "loss: 2.861025  [  500/ 3589]\n",
      "loss: 1.692382  [ 1000/ 3589]\n",
      "loss: 1.474959  [ 1500/ 3589]\n",
      "loss: 1.823869  [ 2000/ 3589]\n",
      "loss: 1.102768  [ 2500/ 3589]\n",
      "loss: 1.460366  [ 3000/ 3589]\n",
      "loss: 1.705823  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7330 \tAverage TrainAcc: 0.3374\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6508 \tAverage ValAcc: 0.3826\n",
      "\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.896274  [    0/ 3589]\n",
      "loss: 1.658047  [  500/ 3589]\n",
      "loss: 1.870591  [ 1000/ 3589]\n",
      "loss: 1.511041  [ 1500/ 3589]\n",
      "loss: 1.371191  [ 2000/ 3589]\n",
      "loss: 1.807998  [ 2500/ 3589]\n",
      "loss: 1.560963  [ 3000/ 3589]\n",
      "loss: 1.921276  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7382 \tAverage TrainAcc: 0.3362\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6746 \tAverage ValAcc: 0.3335\n",
      "\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.543137  [    0/ 3589]\n",
      "loss: 1.555558  [  500/ 3589]\n",
      "loss: 1.797873  [ 1000/ 3589]\n",
      "loss: 1.141046  [ 1500/ 3589]\n",
      "loss: 1.603838  [ 2000/ 3589]\n",
      "loss: 1.275677  [ 2500/ 3589]\n",
      "loss: 1.833822  [ 3000/ 3589]\n",
      "loss: 1.785284  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7334 \tAverage TrainAcc: 0.3380\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5332 \tAverage ValAcc: 0.4007\n",
      "\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.315320  [    0/ 3589]\n",
      "loss: 1.862669  [  500/ 3589]\n",
      "loss: 2.081940  [ 1000/ 3589]\n",
      "loss: 1.527376  [ 1500/ 3589]\n",
      "loss: 1.337319  [ 2000/ 3589]\n",
      "loss: 1.520013  [ 2500/ 3589]\n",
      "loss: 1.476915  [ 3000/ 3589]\n",
      "loss: 1.741389  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7344 \tAverage TrainAcc: 0.3393\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6567 \tAverage ValAcc: 0.3625\n",
      "\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.724681  [    0/ 3589]\n",
      "loss: 1.704091  [  500/ 3589]\n",
      "loss: 1.749056  [ 1000/ 3589]\n",
      "loss: 1.267995  [ 1500/ 3589]\n",
      "loss: 1.047418  [ 2000/ 3589]\n",
      "loss: 1.619238  [ 2500/ 3589]\n",
      "loss: 1.233647  [ 3000/ 3589]\n",
      "loss: 2.077116  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7404 \tAverage TrainAcc: 0.3364\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6966 \tAverage ValAcc: 0.3801\n",
      "\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.276345  [    0/ 3589]\n",
      "loss: 1.441100  [  500/ 3589]\n",
      "loss: 1.761778  [ 1000/ 3589]\n",
      "loss: 2.227108  [ 1500/ 3589]\n",
      "loss: 1.306502  [ 2000/ 3589]\n",
      "loss: 2.030693  [ 2500/ 3589]\n",
      "loss: 1.483747  [ 3000/ 3589]\n",
      "loss: 2.411347  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7384 \tAverage TrainAcc: 0.3359\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5623 \tAverage ValAcc: 0.4023\n",
      "\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.307344  [    0/ 3589]\n",
      "loss: 1.307011  [  500/ 3589]\n",
      "loss: 2.055612  [ 1000/ 3589]\n",
      "loss: 1.336095  [ 1500/ 3589]\n",
      "loss: 1.929260  [ 2000/ 3589]\n",
      "loss: 1.431356  [ 2500/ 3589]\n",
      "loss: 1.539144  [ 3000/ 3589]\n",
      "loss: 2.442445  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7349 \tAverage TrainAcc: 0.3349\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6398 \tAverage ValAcc: 0.3656\n",
      "\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.160857  [    0/ 3589]\n",
      "loss: 2.169077  [  500/ 3589]\n",
      "loss: 1.492778  [ 1000/ 3589]\n",
      "loss: 2.007858  [ 1500/ 3589]\n",
      "loss: 1.539772  [ 2000/ 3589]\n",
      "loss: 1.922710  [ 2500/ 3589]\n",
      "loss: 1.609878  [ 3000/ 3589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.505315  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7315 \tAverage TrainAcc: 0.3396\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6559 \tAverage ValAcc: 0.3943\n",
      "\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.218401  [    0/ 3589]\n",
      "loss: 1.213001  [  500/ 3589]\n",
      "loss: 1.481663  [ 1000/ 3589]\n",
      "loss: 1.845937  [ 1500/ 3589]\n",
      "loss: 1.708820  [ 2000/ 3589]\n",
      "loss: 1.427733  [ 2500/ 3589]\n",
      "loss: 1.373033  [ 3000/ 3589]\n",
      "loss: 1.969328  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7205 \tAverage TrainAcc: 0.3417\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5795 \tAverage ValAcc: 0.3912\n",
      "\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.677276  [    0/ 3589]\n",
      "loss: 1.191614  [  500/ 3589]\n",
      "loss: 1.620971  [ 1000/ 3589]\n",
      "loss: 2.060056  [ 1500/ 3589]\n",
      "loss: 1.371291  [ 2000/ 3589]\n",
      "loss: 1.302285  [ 2500/ 3589]\n",
      "loss: 1.915738  [ 3000/ 3589]\n",
      "loss: 2.127298  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7321 \tAverage TrainAcc: 0.3378\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5566 \tAverage ValAcc: 0.4007\n",
      "\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.339363  [    0/ 3589]\n",
      "loss: 1.322540  [  500/ 3589]\n",
      "loss: 1.793406  [ 1000/ 3589]\n",
      "loss: 1.730432  [ 1500/ 3589]\n",
      "loss: 1.940881  [ 2000/ 3589]\n",
      "loss: 1.564670  [ 2500/ 3589]\n",
      "loss: 1.177740  [ 3000/ 3589]\n",
      "loss: 1.361473  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7331 \tAverage TrainAcc: 0.3386\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5470 \tAverage ValAcc: 0.4071\n",
      "\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.898292  [    0/ 3589]\n",
      "loss: 1.718024  [  500/ 3589]\n",
      "loss: 1.116891  [ 1000/ 3589]\n",
      "loss: 1.902984  [ 1500/ 3589]\n",
      "loss: 1.419913  [ 2000/ 3589]\n",
      "loss: 1.731363  [ 2500/ 3589]\n",
      "loss: 1.750248  [ 3000/ 3589]\n",
      "loss: 1.705203  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.6177 \tAverage TrainAcc: 0.3678\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5243 \tAverage ValAcc: 0.4054\n",
      "\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.637967  [    0/ 3589]\n",
      "loss: 1.875573  [  500/ 3589]\n",
      "loss: 1.721271  [ 1000/ 3589]\n",
      "loss: 1.290651  [ 1500/ 3589]\n",
      "loss: 1.657078  [ 2000/ 3589]\n",
      "loss: 1.143330  [ 2500/ 3589]\n",
      "loss: 1.321510  [ 3000/ 3589]\n",
      "loss: 1.672925  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.6048 \tAverage TrainAcc: 0.3733\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5165 \tAverage ValAcc: 0.4185\n",
      "\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.620449  [    0/ 3589]\n",
      "loss: 1.367635  [  500/ 3589]\n",
      "loss: 1.871081  [ 1000/ 3589]\n",
      "loss: 1.496495  [ 1500/ 3589]\n",
      "loss: 1.924862  [ 2000/ 3589]\n",
      "loss: 1.150006  [ 2500/ 3589]\n",
      "loss: 2.057248  [ 3000/ 3589]\n",
      "loss: 1.043648  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.5996 \tAverage TrainAcc: 0.3754\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5086 \tAverage ValAcc: 0.4238\n",
      "\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.894011  [    0/ 3589]\n",
      "loss: 1.683371  [  500/ 3589]\n",
      "loss: 1.729295  [ 1000/ 3589]\n",
      "loss: 1.465400  [ 1500/ 3589]\n",
      "loss: 1.409356  [ 2000/ 3589]\n",
      "loss: 1.732517  [ 2500/ 3589]\n",
      "loss: 1.912588  [ 3000/ 3589]\n",
      "loss: 1.428862  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.5895 \tAverage TrainAcc: 0.3770\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5112 \tAverage ValAcc: 0.4168\n",
      "\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.967211  [    0/ 3589]\n",
      "loss: 1.304906  [  500/ 3589]\n",
      "loss: 1.166052  [ 1000/ 3589]\n",
      "loss: 1.371938  [ 1500/ 3589]\n",
      "loss: 1.437122  [ 2000/ 3589]\n",
      "loss: 1.429914  [ 2500/ 3589]\n",
      "loss: 1.202965  [ 3000/ 3589]\n",
      "loss: 1.429020  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.5889 \tAverage TrainAcc: 0.3790\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5072 \tAverage ValAcc: 0.4149\n",
      "\n",
      "\n",
      "Training complete in 78m 60s\n"
     ]
    }
   ],
   "source": [
    "# Start training and val loops\n",
    "\n",
    "since = time.time()\n",
    "for epoch in range(cfg.EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    \n",
    "    # Train\n",
    "    ave_train_loss, ave_train_acc = train_epoch(epoch, model, train_loader, criterion, optimizer, device, 500, writer)\n",
    "    print(f'Average TrainLoss: {ave_train_loss:.4f} \\tAverage TrainAcc: {ave_train_acc:.4f}')\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"Evaluation:\")\n",
    "    ave_val_loss, ave_val_acc = val_epoch(epoch, model, val_loader, criterion, device, writer)\n",
    "    print(f'Average ValLoss: {ave_val_loss:.4f} \\tAverage ValAcc: {ave_val_acc:.4f}')\n",
    "    \n",
    "    # log metrics at every epoch\n",
    "#     if writer:\n",
    "#         writer.add_scalar(\"train_loss\", ave_train_loss, epoch)\n",
    "#         writer.add_scalar(\"train_accuracy\", ave_train_acc, epoch)\n",
    "#         writer.add_scalar(\"val_loss\", ave_val_loss, epoch)\n",
    "#         writer.add_scalar(\"val_accuracy\", ave_val_acc, epoch)\n",
    "    \n",
    "    # schedule lr\n",
    "    scheduler.step(val_loss)\n",
    "    print(\"\\n\")\n",
    "        \n",
    "time_elapsed = time.time() - since\n",
    "print(f\"Training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9478f0f3",
   "metadata": {},
   "source": [
    "# Evaluating on Unseen Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb796757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average TestLoss: 1.5070 \tAverage TestAcc: 0.4138\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_corrects = []\n",
    "\n",
    "model.eval()\n",
    "# Iterate over data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # prediction\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # calculate loss\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # statistics\n",
    "        test_losses.append(loss.item())\n",
    "        test_corrects.append(torch.sum(preds == labels.data).item())\n",
    "\n",
    "    ave_test_loss = sum(test_losses)/len(test_losses)\n",
    "    ave_test_acc = sum(test_corrects)/len(test_loader.dataset)\n",
    "    \n",
    "print(f'Average TestLoss: {ave_test_loss:.4f} \\tAverage TestAcc: {ave_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74bbc1",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa15c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_name = f\"model_epoch{cfg.EPOCHS}_lr{cfg.LR}_batch{cfg.BATCH_SIZE}.pt\"\n",
    "torch.save(model.state_dict(), cfg.MODEL_DIR + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2785c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aff473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
