{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eabf088",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a164957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc15f8b7",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b01497a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"angry\", \n",
    "    \"disgust\", \n",
    "    \"fear\", \n",
    "    \"happy\", \n",
    "    \"sad\", \n",
    "    \"surprise\", \n",
    "    \"neutral\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "85ac1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # dataset\n",
    "    TRAIN_DS_PATH = './dataset/train.csv'\n",
    "    VAL_DS_PATH = './dataset/val.csv'\n",
    "    TEST_DS_PATH = './dataset/finaltest.csv'\n",
    "    \n",
    "    # images dir\n",
    "    TRAIN_IMG_DIR = './dataset/train/'\n",
    "    VAL_IMG_DIR  = './dataset/val/'\n",
    "    TEST_IMG_DIR  = './dataset/finaltest/'\n",
    "    \n",
    "    # training hyperparams\n",
    "    EPOCHS = 25\n",
    "    LR = 1e-3\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0\n",
    "    SHUFFLE = True\n",
    "    \n",
    "    # saved model path\n",
    "    MODEL_DIR = './model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "3a43450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "28f5e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard writer\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "writer = SummaryWriter(f\"runs/{current_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd927d2d",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2e5d1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/omarsayed7/Deep-Emotion/blob/master/data_loaders.py\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, datatype, transform):\n",
    "        '''\n",
    "        Pytorch Dataset class\n",
    "        params:-\n",
    "                 csv_file : the path of the csv file    (train, validation, test)\n",
    "                 img_dir  : the directory of the images (train, validation, test)\n",
    "                 datatype : data type for the dataset   (train, val, test)\n",
    "                 transform: pytorch transformation over the data\n",
    "        return :-\n",
    "                 image, labels\n",
    "        '''\n",
    "        self.csv_file = pd.read_csv(csv_file)\n",
    "        self.labels = self.csv_file['emotion']\n",
    "        self.img_dir = img_dir\n",
    "        self.datatype = datatype\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img = Image.open(self.img_dir + self.datatype + str(idx) + '.jpg').convert('RGB')\n",
    "        labels = np.array(self.labels[idx])\n",
    "        labels = torch.from_numpy(labels).long()\n",
    "\n",
    "        if self.transform :\n",
    "            img = self.transform(img)\n",
    "        return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "653bb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranformations\n",
    "transformation= transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "#     transforms.Grayscale(3), # no need this since you .convert(\"RGB\") at __getitem__\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "60019473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset = EmotionDataset(\n",
    "    csv_file=cfg.TRAIN_DS_PATH, \n",
    "    img_dir=cfg.TRAIN_IMG_DIR,\n",
    "    datatype='train',\n",
    "    transform=transformation\n",
    ")\n",
    "\n",
    "validation_dataset = EmotionDataset(\n",
    "    csv_file=cfg.VAL_DS_PATH, \n",
    "    img_dir=cfg.VAL_IMG_DIR,\n",
    "    datatype='val',\n",
    "    transform = transformation\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = EmotionDataset(\n",
    "    csv_file=cfg.TEST_DS_PATH, \n",
    "    img_dir=cfg.TEST_IMG_DIR,\n",
    "    datatype='finaltest',\n",
    "    transform = transformation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "595dd83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=cfg.BATCH_SIZE, \n",
    "    shuffle =cfg.SHUFFLE, \n",
    "    num_workers=cfg.NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=cfg.BATCH_SIZE, \n",
    "    shuffle =cfg.SHUFFLE, \n",
    "    num_workers=cfg.NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=cfg.BATCH_SIZE, \n",
    "    shuffle =cfg.SHUFFLE, \n",
    "    num_workers=cfg.NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c161e",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "8b5bfea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, train_loader, criterion, optimizer, device, log_every_n_batches=0, writer=None):\n",
    "    \"\"\"Train the model for 1 epoch\n",
    "    Args:\n",
    "        epoch: Current training epoch\n",
    "        model: nn.Module\n",
    "        train_loader: train DataLoader\n",
    "        criterion: callable loss function\n",
    "        optimizer: pytorch optimizer\n",
    "        device: torch.device\n",
    "        log_every_n_batches: Metrics will log every n batches\n",
    "        writer: Tensorboard SummaryWriter\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Float, Float]\n",
    "        average train loss and average train accuracy for current epoch\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    train_corrects = []\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over data.\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # prediction\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # calculate loss\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # number of corrects\n",
    "        n_corrects = torch.sum(preds == labels.data).item()\n",
    "        \n",
    "        # log every n batches\n",
    "        if log_every_n_batches and (batch_idx % log_every_n_batches == 0):\n",
    "            print(f\"loss: {loss.item():>7f}  [{batch_idx:>5d}/{len(train_loader):>5d}]\")\n",
    "            if writer:\n",
    "                writer.add_scalar(\n",
    "                    \"train_loss\", \n",
    "                    loss.item(),\n",
    "                    global_step=epoch*len(train_loader)*batch_idx\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"train_accuracy\",\n",
    "                    n_corrects/len(),\n",
    "                    global_step=epoch*len(train_loader)*batch_idx\n",
    "                )\n",
    "            \n",
    "        # accumulate for average metric later calculation\n",
    "        train_losses.append(loss.item())\n",
    "        train_corrects.append(n_corrects)\n",
    "        \n",
    "    ave_train_loss = sum(train_losses)/len(train_losses)\n",
    "    ave_train_accuracy = sum(train_corrects)/len(train_loader.dataset)      \n",
    "\n",
    "    return ave_train_loss, ave_train_accuracy\n",
    "\n",
    "\n",
    "def val_epoch(epoch, model, val_loader, criterion, device, log_every_n_steps=0, writer=None):\n",
    "    \"\"\"Validate the model for 1 epoch\n",
    "    Args:\n",
    "        epoch: Current validation epoch\n",
    "        model: nn.Module\n",
    "        val_loader: val DataLoader\n",
    "        criterion: callable loss function\n",
    "        device: torch.device\n",
    "        log_every_n_steps: Metrics will log every n steps\n",
    "        writer: Tensorboard SummaryWriter\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Float, Float]\n",
    "        average val loss and average val accuracy for current epoch\n",
    "    \"\"\"\n",
    "\n",
    "    val_losses = []\n",
    "    val_corrects = []\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over data\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # prediction\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # calculate loss\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # number of corrects\n",
    "            n_corrects = torch.sum(preds == labels.data).item()\n",
    "            \n",
    "            # log every n steps\n",
    "            if writer and log_every_n_steps and (batch_idx % log_every_n_steps == 0):\n",
    "                writer.add_scalar(\n",
    "                    \"val_loss\", \n",
    "                    loss.item(),\n",
    "                    global_step=epoch*len(val_loader)*batch_idx\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"val_accuracy\",\n",
    "                    n_corrects/100,\n",
    "                    global_step=epoch*len(val_loader)*batch_idx\n",
    "                )\n",
    "                \n",
    "            # accumulate for average metric later calculation\n",
    "            val_losses.append(loss.item())\n",
    "            val_corrects.append(n_corrects)\n",
    "            \n",
    "        ave_val_loss = sum(val_losses)/len(val_losses)\n",
    "        ave_val_accuracy = sum(val_corrects)/len(val_loader.dataset)\n",
    "\n",
    "    return ave_val_loss, ave_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241625b",
   "metadata": {},
   "source": [
    "# Training and Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "19b36628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "model = models.mobilenet.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "888b5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_features, len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "efe3cfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# transfer to cuda device if any\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1c76f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, optimizer and scheduler \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=cfg.LR)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a3a93309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.825439  [    0/ 3589]\n",
      "loss: 2.576636  [  100/ 3589]\n",
      "loss: 1.996446  [  200/ 3589]\n",
      "loss: 1.742562  [  300/ 3589]\n",
      "loss: 1.605074  [  400/ 3589]\n",
      "loss: 1.537800  [  500/ 3589]\n",
      "loss: 1.332143  [  600/ 3589]\n",
      "loss: 1.476863  [  700/ 3589]\n",
      "loss: 1.994385  [  800/ 3589]\n",
      "loss: 1.566823  [  900/ 3589]\n",
      "loss: 1.651812  [ 1000/ 3589]\n",
      "loss: 1.880280  [ 1100/ 3589]\n",
      "loss: 2.642682  [ 1200/ 3589]\n",
      "loss: 1.744510  [ 1300/ 3589]\n",
      "loss: 1.695707  [ 1400/ 3589]\n",
      "loss: 1.658113  [ 1500/ 3589]\n",
      "loss: 1.534049  [ 1600/ 3589]\n",
      "loss: 1.842635  [ 1700/ 3589]\n",
      "loss: 2.410210  [ 1800/ 3589]\n",
      "loss: 1.508202  [ 1900/ 3589]\n",
      "loss: 1.286600  [ 2000/ 3589]\n",
      "loss: 1.707210  [ 2100/ 3589]\n",
      "loss: 1.254705  [ 2200/ 3589]\n",
      "loss: 1.658910  [ 2300/ 3589]\n",
      "loss: 1.682789  [ 2400/ 3589]\n",
      "loss: 1.848973  [ 2500/ 3589]\n",
      "loss: 1.489694  [ 2600/ 3589]\n",
      "loss: 1.819366  [ 2700/ 3589]\n",
      "loss: 1.994417  [ 2800/ 3589]\n",
      "loss: 2.100586  [ 2900/ 3589]\n",
      "loss: 1.927252  [ 3000/ 3589]\n",
      "loss: 1.702443  [ 3100/ 3589]\n",
      "loss: 1.625972  [ 3200/ 3589]\n",
      "loss: 1.342059  [ 3300/ 3589]\n",
      "loss: 1.692427  [ 3400/ 3589]\n",
      "loss: 1.709166  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7563 \tAverage TrainAcc: 0.3104\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6043 \tAverage ValAcc: 0.3775\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.890028  [    0/ 3589]\n",
      "loss: 1.458062  [  100/ 3589]\n",
      "loss: 1.441051  [  200/ 3589]\n",
      "loss: 1.974638  [  300/ 3589]\n",
      "loss: 1.677638  [  400/ 3589]\n",
      "loss: 1.688652  [  500/ 3589]\n",
      "loss: 1.857118  [  600/ 3589]\n",
      "loss: 1.390171  [  700/ 3589]\n",
      "loss: 1.234120  [  800/ 3589]\n",
      "loss: 2.186258  [  900/ 3589]\n",
      "loss: 1.289859  [ 1000/ 3589]\n",
      "loss: 1.334918  [ 1100/ 3589]\n",
      "loss: 1.407720  [ 1200/ 3589]\n",
      "loss: 1.308632  [ 1300/ 3589]\n",
      "loss: 1.616285  [ 1400/ 3589]\n",
      "loss: 1.858350  [ 1500/ 3589]\n",
      "loss: 1.891019  [ 1600/ 3589]\n",
      "loss: 1.778767  [ 1700/ 3589]\n",
      "loss: 1.702826  [ 1800/ 3589]\n",
      "loss: 2.547269  [ 1900/ 3589]\n",
      "loss: 1.953017  [ 2000/ 3589]\n",
      "loss: 1.156632  [ 2100/ 3589]\n",
      "loss: 2.862688  [ 2200/ 3589]\n",
      "loss: 1.284435  [ 2300/ 3589]\n",
      "loss: 1.924013  [ 2400/ 3589]\n",
      "loss: 1.407650  [ 2500/ 3589]\n",
      "loss: 1.797325  [ 2600/ 3589]\n",
      "loss: 1.330996  [ 2700/ 3589]\n",
      "loss: 1.825467  [ 2800/ 3589]\n",
      "loss: 1.766662  [ 2900/ 3589]\n",
      "loss: 1.923809  [ 3000/ 3589]\n",
      "loss: 2.430464  [ 3100/ 3589]\n",
      "loss: 1.202594  [ 3200/ 3589]\n",
      "loss: 1.489322  [ 3300/ 3589]\n",
      "loss: 1.732829  [ 3400/ 3589]\n",
      "loss: 1.801950  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7373 \tAverage TrainAcc: 0.3339\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6166 \tAverage ValAcc: 0.3787\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.301027  [    0/ 3589]\n",
      "loss: 1.692189  [  100/ 3589]\n",
      "loss: 1.879593  [  200/ 3589]\n",
      "loss: 1.539888  [  300/ 3589]\n",
      "loss: 1.732485  [  400/ 3589]\n",
      "loss: 2.266443  [  500/ 3589]\n",
      "loss: 2.469285  [  600/ 3589]\n",
      "loss: 1.769162  [  700/ 3589]\n",
      "loss: 2.385908  [  800/ 3589]\n",
      "loss: 1.158644  [  900/ 3589]\n",
      "loss: 2.176005  [ 1000/ 3589]\n",
      "loss: 1.743605  [ 1100/ 3589]\n",
      "loss: 1.315325  [ 1200/ 3589]\n",
      "loss: 1.073760  [ 1300/ 3589]\n",
      "loss: 1.193379  [ 1400/ 3589]\n",
      "loss: 1.263044  [ 1500/ 3589]\n",
      "loss: 1.500392  [ 1600/ 3589]\n",
      "loss: 1.390344  [ 1700/ 3589]\n",
      "loss: 2.232896  [ 1800/ 3589]\n",
      "loss: 2.022940  [ 1900/ 3589]\n",
      "loss: 2.638337  [ 2000/ 3589]\n",
      "loss: 1.990281  [ 2100/ 3589]\n",
      "loss: 1.521061  [ 2200/ 3589]\n",
      "loss: 1.519051  [ 2300/ 3589]\n",
      "loss: 1.700345  [ 2400/ 3589]\n",
      "loss: 1.949558  [ 2500/ 3589]\n",
      "loss: 1.607383  [ 2600/ 3589]\n",
      "loss: 1.152352  [ 2700/ 3589]\n",
      "loss: 1.871598  [ 2800/ 3589]\n",
      "loss: 1.368442  [ 2900/ 3589]\n",
      "loss: 2.137284  [ 3000/ 3589]\n",
      "loss: 2.070647  [ 3100/ 3589]\n",
      "loss: 1.911688  [ 3200/ 3589]\n",
      "loss: 1.461515  [ 3300/ 3589]\n",
      "loss: 1.342993  [ 3400/ 3589]\n",
      "loss: 1.918592  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7365 \tAverage TrainAcc: 0.3321\n",
      "Evaluation:\n",
      "Average ValLoss: 1.7063 \tAverage ValAcc: 0.3547\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.968090  [    0/ 3589]\n",
      "loss: 2.043231  [  100/ 3589]\n",
      "loss: 1.881422  [  200/ 3589]\n",
      "loss: 1.331596  [  300/ 3589]\n",
      "loss: 2.421442  [  400/ 3589]\n",
      "loss: 1.488934  [  500/ 3589]\n",
      "loss: 1.518738  [  600/ 3589]\n",
      "loss: 1.449302  [  700/ 3589]\n",
      "loss: 2.117060  [  800/ 3589]\n",
      "loss: 1.537612  [  900/ 3589]\n",
      "loss: 1.089648  [ 1000/ 3589]\n",
      "loss: 2.147839  [ 1100/ 3589]\n",
      "loss: 1.118016  [ 1200/ 3589]\n",
      "loss: 1.270849  [ 1300/ 3589]\n",
      "loss: 1.385252  [ 1400/ 3589]\n",
      "loss: 1.402369  [ 1500/ 3589]\n",
      "loss: 1.388201  [ 1600/ 3589]\n",
      "loss: 1.711230  [ 1700/ 3589]\n",
      "loss: 1.389770  [ 1800/ 3589]\n",
      "loss: 1.116563  [ 1900/ 3589]\n",
      "loss: 2.542815  [ 2000/ 3589]\n",
      "loss: 1.613366  [ 2100/ 3589]\n",
      "loss: 1.291011  [ 2200/ 3589]\n",
      "loss: 1.756854  [ 2300/ 3589]\n",
      "loss: 2.049592  [ 2400/ 3589]\n",
      "loss: 1.931713  [ 2500/ 3589]\n",
      "loss: 1.542313  [ 2600/ 3589]\n",
      "loss: 1.847604  [ 2700/ 3589]\n",
      "loss: 1.218888  [ 2800/ 3589]\n",
      "loss: 2.541111  [ 2900/ 3589]\n",
      "loss: 1.456485  [ 3000/ 3589]\n",
      "loss: 2.132361  [ 3100/ 3589]\n",
      "loss: 1.507843  [ 3200/ 3589]\n",
      "loss: 1.843532  [ 3300/ 3589]\n",
      "loss: 1.658468  [ 3400/ 3589]\n",
      "loss: 1.461194  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7327 \tAverage TrainAcc: 0.3402\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6139 \tAverage ValAcc: 0.3951\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.408194  [    0/ 3589]\n",
      "loss: 1.523045  [  100/ 3589]\n",
      "loss: 1.800246  [  200/ 3589]\n",
      "loss: 1.411113  [  300/ 3589]\n",
      "loss: 1.511168  [  400/ 3589]\n",
      "loss: 1.978677  [  500/ 3589]\n",
      "loss: 2.225306  [  600/ 3589]\n",
      "loss: 2.045766  [  700/ 3589]\n",
      "loss: 1.527707  [  800/ 3589]\n",
      "loss: 1.445409  [  900/ 3589]\n",
      "loss: 2.523999  [ 1000/ 3589]\n",
      "loss: 2.428258  [ 1100/ 3589]\n",
      "loss: 1.796674  [ 1200/ 3589]\n",
      "loss: 1.463388  [ 1300/ 3589]\n",
      "loss: 1.623125  [ 1400/ 3589]\n",
      "loss: 1.810266  [ 1500/ 3589]\n",
      "loss: 1.629417  [ 1600/ 3589]\n",
      "loss: 1.561097  [ 1700/ 3589]\n",
      "loss: 1.955175  [ 1800/ 3589]\n",
      "loss: 1.635472  [ 1900/ 3589]\n",
      "loss: 1.540990  [ 2000/ 3589]\n",
      "loss: 1.758259  [ 2100/ 3589]\n",
      "loss: 1.495659  [ 2200/ 3589]\n",
      "loss: 1.451790  [ 2300/ 3589]\n",
      "loss: 1.938262  [ 2400/ 3589]\n",
      "loss: 2.036578  [ 2500/ 3589]\n",
      "loss: 1.456689  [ 2600/ 3589]\n",
      "loss: 1.826873  [ 2700/ 3589]\n",
      "loss: 1.559269  [ 2800/ 3589]\n",
      "loss: 1.879194  [ 2900/ 3589]\n",
      "loss: 1.277720  [ 3000/ 3589]\n",
      "loss: 1.633010  [ 3100/ 3589]\n",
      "loss: 1.511996  [ 3200/ 3589]\n",
      "loss: 1.445999  [ 3300/ 3589]\n",
      "loss: 1.286286  [ 3400/ 3589]\n",
      "loss: 1.782088  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7368 \tAverage TrainAcc: 0.3387\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6665 \tAverage ValAcc: 0.3608\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.033788  [    0/ 3589]\n",
      "loss: 1.797887  [  100/ 3589]\n",
      "loss: 1.775231  [  200/ 3589]\n",
      "loss: 1.202111  [  300/ 3589]\n",
      "loss: 1.074118  [  400/ 3589]\n",
      "loss: 1.653366  [  500/ 3589]\n",
      "loss: 1.179029  [  600/ 3589]\n",
      "loss: 2.211790  [  700/ 3589]\n",
      "loss: 2.470048  [  800/ 3589]\n",
      "loss: 0.938357  [  900/ 3589]\n",
      "loss: 1.240704  [ 1000/ 3589]\n",
      "loss: 1.487456  [ 1100/ 3589]\n",
      "loss: 1.570666  [ 1200/ 3589]\n",
      "loss: 1.645095  [ 1300/ 3589]\n",
      "loss: 2.056391  [ 1400/ 3589]\n",
      "loss: 2.495336  [ 1500/ 3589]\n",
      "loss: 0.910322  [ 1600/ 3589]\n",
      "loss: 1.952281  [ 1700/ 3589]\n",
      "loss: 1.463351  [ 1800/ 3589]\n",
      "loss: 2.104967  [ 1900/ 3589]\n",
      "loss: 2.146909  [ 2000/ 3589]\n",
      "loss: 1.933944  [ 2100/ 3589]\n",
      "loss: 2.025542  [ 2200/ 3589]\n",
      "loss: 2.007448  [ 2300/ 3589]\n",
      "loss: 1.470006  [ 2400/ 3589]\n",
      "loss: 1.350981  [ 2500/ 3589]\n",
      "loss: 2.032101  [ 2600/ 3589]\n",
      "loss: 2.413342  [ 2700/ 3589]\n",
      "loss: 2.601671  [ 2800/ 3589]\n",
      "loss: 1.659060  [ 2900/ 3589]\n",
      "loss: 2.009567  [ 3000/ 3589]\n",
      "loss: 2.186151  [ 3100/ 3589]\n",
      "loss: 2.155053  [ 3200/ 3589]\n",
      "loss: 2.387795  [ 3300/ 3589]\n",
      "loss: 1.855123  [ 3400/ 3589]\n",
      "loss: 1.610777  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7333 \tAverage TrainAcc: 0.3362\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5776 \tAverage ValAcc: 0.3945\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.934121  [    0/ 3589]\n",
      "loss: 1.126847  [  100/ 3589]\n",
      "loss: 1.082658  [  200/ 3589]\n",
      "loss: 1.657222  [  300/ 3589]\n",
      "loss: 1.806853  [  400/ 3589]\n",
      "loss: 2.007417  [  500/ 3589]\n",
      "loss: 2.064857  [  600/ 3589]\n",
      "loss: 2.111843  [  700/ 3589]\n",
      "loss: 1.364164  [  800/ 3589]\n",
      "loss: 1.483345  [  900/ 3589]\n",
      "loss: 1.576223  [ 1000/ 3589]\n",
      "loss: 1.768963  [ 1100/ 3589]\n",
      "loss: 1.494825  [ 1200/ 3589]\n",
      "loss: 1.581832  [ 1300/ 3589]\n",
      "loss: 1.423769  [ 1400/ 3589]\n",
      "loss: 1.680702  [ 1500/ 3589]\n",
      "loss: 2.975344  [ 1600/ 3589]\n",
      "loss: 1.877920  [ 1700/ 3589]\n",
      "loss: 1.185707  [ 1800/ 3589]\n",
      "loss: 1.866764  [ 1900/ 3589]\n",
      "loss: 1.985609  [ 2000/ 3589]\n",
      "loss: 1.949112  [ 2100/ 3589]\n",
      "loss: 2.280220  [ 2200/ 3589]\n",
      "loss: 1.785342  [ 2300/ 3589]\n",
      "loss: 2.044151  [ 2400/ 3589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.863939  [ 2500/ 3589]\n",
      "loss: 1.058128  [ 2600/ 3589]\n",
      "loss: 2.005748  [ 2700/ 3589]\n",
      "loss: 1.880872  [ 2800/ 3589]\n",
      "loss: 1.233933  [ 2900/ 3589]\n",
      "loss: 1.419806  [ 3000/ 3589]\n",
      "loss: 1.704870  [ 3100/ 3589]\n",
      "loss: 0.668279  [ 3200/ 3589]\n",
      "loss: 1.384274  [ 3300/ 3589]\n",
      "loss: 1.449718  [ 3400/ 3589]\n",
      "loss: 1.786287  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7382 \tAverage TrainAcc: 0.3381\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6472 \tAverage ValAcc: 0.3736\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.677179  [    0/ 3589]\n",
      "loss: 1.381290  [  100/ 3589]\n",
      "loss: 2.129277  [  200/ 3589]\n",
      "loss: 1.706321  [  300/ 3589]\n",
      "loss: 2.044433  [  400/ 3589]\n",
      "loss: 1.633841  [  500/ 3589]\n",
      "loss: 2.031755  [  600/ 3589]\n",
      "loss: 1.493969  [  700/ 3589]\n",
      "loss: 1.588589  [  800/ 3589]\n",
      "loss: 1.515598  [  900/ 3589]\n",
      "loss: 1.692832  [ 1000/ 3589]\n",
      "loss: 2.009440  [ 1100/ 3589]\n",
      "loss: 2.605177  [ 1200/ 3589]\n",
      "loss: 1.541031  [ 1300/ 3589]\n",
      "loss: 1.597568  [ 1400/ 3589]\n",
      "loss: 1.865659  [ 1500/ 3589]\n",
      "loss: 2.098863  [ 1600/ 3589]\n",
      "loss: 1.462942  [ 1700/ 3589]\n",
      "loss: 1.336813  [ 1800/ 3589]\n",
      "loss: 1.338928  [ 1900/ 3589]\n",
      "loss: 3.003780  [ 2000/ 3589]\n",
      "loss: 1.351148  [ 2100/ 3589]\n",
      "loss: 0.910753  [ 2200/ 3589]\n",
      "loss: 2.316547  [ 2300/ 3589]\n",
      "loss: 1.351974  [ 2400/ 3589]\n",
      "loss: 1.824577  [ 2500/ 3589]\n",
      "loss: 1.698676  [ 2600/ 3589]\n",
      "loss: 1.381416  [ 2700/ 3589]\n",
      "loss: 0.876320  [ 2800/ 3589]\n",
      "loss: 1.625859  [ 2900/ 3589]\n",
      "loss: 1.686422  [ 3000/ 3589]\n",
      "loss: 1.493131  [ 3100/ 3589]\n",
      "loss: 2.629285  [ 3200/ 3589]\n",
      "loss: 2.100194  [ 3300/ 3589]\n",
      "loss: 2.415068  [ 3400/ 3589]\n",
      "loss: 1.402009  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7389 \tAverage TrainAcc: 0.3375\n",
      "Evaluation:\n",
      "Average ValLoss: 1.5714 \tAverage ValAcc: 0.4009\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.085902  [    0/ 3589]\n",
      "loss: 2.500630  [  100/ 3589]\n",
      "loss: 1.687349  [  200/ 3589]\n",
      "loss: 0.676266  [  300/ 3589]\n",
      "loss: 1.134630  [  400/ 3589]\n",
      "loss: 1.465157  [  500/ 3589]\n",
      "loss: 1.041981  [  600/ 3589]\n",
      "loss: 2.101893  [  700/ 3589]\n",
      "loss: 1.519704  [  800/ 3589]\n",
      "loss: 1.847677  [  900/ 3589]\n",
      "loss: 1.923095  [ 1000/ 3589]\n",
      "loss: 2.310797  [ 1100/ 3589]\n",
      "loss: 1.623745  [ 1200/ 3589]\n",
      "loss: 1.379184  [ 1300/ 3589]\n",
      "loss: 2.172203  [ 1400/ 3589]\n",
      "loss: 1.248720  [ 1500/ 3589]\n",
      "loss: 1.844671  [ 1600/ 3589]\n",
      "loss: 2.209209  [ 1700/ 3589]\n",
      "loss: 1.862321  [ 1800/ 3589]\n",
      "loss: 1.416013  [ 1900/ 3589]\n",
      "loss: 1.302632  [ 2000/ 3589]\n",
      "loss: 1.653653  [ 2100/ 3589]\n",
      "loss: 1.572601  [ 2200/ 3589]\n",
      "loss: 1.596496  [ 2300/ 3589]\n",
      "loss: 1.350764  [ 2400/ 3589]\n",
      "loss: 2.670992  [ 2500/ 3589]\n",
      "loss: 1.638833  [ 2600/ 3589]\n",
      "loss: 0.881637  [ 2700/ 3589]\n",
      "loss: 1.328385  [ 2800/ 3589]\n",
      "loss: 1.879165  [ 2900/ 3589]\n",
      "loss: 2.074682  [ 3000/ 3589]\n",
      "loss: 2.350676  [ 3100/ 3589]\n",
      "loss: 1.964366  [ 3200/ 3589]\n",
      "loss: 1.282578  [ 3300/ 3589]\n",
      "loss: 1.625288  [ 3400/ 3589]\n",
      "loss: 2.061718  [ 3500/ 3589]\n",
      "Average TrainLoss: 1.7419 \tAverage TrainAcc: 0.3372\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6540 \tAverage ValAcc: 0.3444\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.854914  [    0/ 3589]\n",
      "loss: 1.020013  [  100/ 3589]\n",
      "loss: 1.748675  [  200/ 3589]\n",
      "loss: 1.370652  [  300/ 3589]\n",
      "loss: 1.755247  [  400/ 3589]\n",
      "loss: 1.736296  [  500/ 3589]\n",
      "loss: 1.189692  [  600/ 3589]\n",
      "loss: 1.288910  [  700/ 3589]\n",
      "loss: 1.336643  [  800/ 3589]\n",
      "loss: 2.148985  [  900/ 3589]\n",
      "loss: 2.056849  [ 1000/ 3589]\n",
      "loss: 1.433673  [ 1100/ 3589]\n",
      "loss: 2.380399  [ 1200/ 3589]\n",
      "loss: 1.544041  [ 1300/ 3589]\n",
      "loss: 1.062707  [ 1400/ 3589]\n",
      "loss: 1.820779  [ 1500/ 3589]\n",
      "loss: 1.622386  [ 1600/ 3589]\n",
      "loss: 1.202790  [ 1700/ 3589]\n",
      "loss: 0.997979  [ 1800/ 3589]\n",
      "loss: 1.559024  [ 1900/ 3589]\n",
      "loss: 1.241366  [ 2000/ 3589]\n",
      "loss: 0.776134  [ 2100/ 3589]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [245]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m ave_train_loss, ave_train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage TrainLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mave_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mAverage TrainAcc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mave_train_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "Input \u001b[1;32mIn [240]\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(epoch, model, train_loader, criterion, optimizer, device, log_every_n_steps, writer)\u001b[0m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Iterate over data.\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     23\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\users\\ivand\\projects\\ai\\emotion-recognition\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\users\\ivand\\projects\\ai\\emotion-recognition\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\users\\ivand\\projects\\ai\\emotion-recognition\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\users\\ivand\\projects\\ai\\emotion-recognition\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [236]\u001b[0m, in \u001b[0;36mEmotionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     28\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(labels)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform :\n\u001b[1;32m---> 31\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, labels\n",
      "File \u001b[1;32mc:\\users\\ivand\\projects\\ai\\emotion-recognition\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\users\\ivand\\projects\\ai\\emotion-recognition\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\ivand\\projects\\ai\\emotion-recognition\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:349\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\ivand\\projects\\ai\\emotion-recognition\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:436\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    434\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    435\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39msize, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, max_size\u001b[38;5;241m=\u001b[39mmax_size, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\users\\ivand\\projects\\ai\\emotion-recognition\\venv\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:258\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[0;32m    255\u001b[0m             new_short, new_long \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(max_size \u001b[38;5;241m*\u001b[39m new_short \u001b[38;5;241m/\u001b[39m new_long), max_size\n\u001b[0;32m    257\u001b[0m     new_w, new_h \u001b[38;5;241m=\u001b[39m (new_short, new_long) \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m h \u001b[38;5;28;01melse\u001b[39;00m (new_long, new_short)\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_h\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\ivand\\projects\\ai\\emotion-recognition\\venv\\lib\\site-packages\\PIL\\Image.py:1980\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   1972\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   1973\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1974\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   1975\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   1976\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   1977\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   1978\u001b[0m         )\n\u001b[1;32m-> 1980\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training and val loops\n",
    "\n",
    "since = time.time()\n",
    "for epoch in range(cfg.EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    \n",
    "    # Train\n",
    "    ave_train_loss, ave_train_acc = train_epoch(epoch, model, train_loader, criterion, optimizer, device, 100, writer)\n",
    "    print(f'Average TrainLoss: {ave_train_loss:.4f} \\tAverage TrainAcc: {ave_train_acc:.4f}')\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"Evaluation:\")\n",
    "    ave_val_loss, ave_val_acc = val_epoch(epoch, model, val_loader, criterion, device, 100, writer)\n",
    "    print(f'Average ValLoss: {ave_val_loss:.4f} \\tAverage ValAcc: {ave_val_acc:.4f}')\n",
    "    \n",
    "    # log metrics at every epoch\n",
    "#     if writer:\n",
    "#         writer.add_scalar(\"train_loss\", ave_train_loss, epoch)\n",
    "#         writer.add_scalar(\"train_accuracy\", ave_train_acc, epoch)\n",
    "#         writer.add_scalar(\"val_loss\", ave_val_loss, epoch)\n",
    "#         writer.add_scalar(\"val_accuracy\", ave_val_acc, epoch)\n",
    "    \n",
    "    # schedule lr\n",
    "    scheduler.step(ave_val_loss)\n",
    "    print(\"\\n\")\n",
    "        \n",
    "time_elapsed = time.time() - since\n",
    "print(f\"Training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c1110",
   "metadata": {},
   "source": [
    "# Evaluating on Unseen Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "test_corrects = []\n",
    "\n",
    "model.eval()\n",
    "# Iterate over data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # prediction\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # calculate loss\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # statistics\n",
    "        test_losses.append(loss.item())\n",
    "        test_corrects.append(torch.sum(preds == labels.data).item())\n",
    "\n",
    "    ave_test_loss = sum(test_losses)/len(test_losses)\n",
    "    ave_test_acc = sum(test_corrects)/len(test_loader.dataset)\n",
    "    \n",
    "print(f'Average TestLoss: {ave_test_loss:.4f} \\tAverage TestAcc: {ave_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465855c0",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "model_name = f\"model_{current_time}_epoch{cfg.EPOCHS}_lr{cfg.LR}_batch{cfg.BATCH_SIZE}_acc{ave_test_acc:.3f}.pt\"\n",
    "torch.save(model.state_dict(), cfg.MODEL_DIR + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2785c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aff473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
