{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eabf088",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a164957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc15f8b7",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a277bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"angry\", \n",
    "    \"disgust\", \n",
    "    \"fear\", \n",
    "    \"happy\", \n",
    "    \"sad\", \n",
    "    \"surprise\", \n",
    "    \"neutral\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ac1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # dataset\n",
    "    TRAIN_DS_PATH = './dataset/train.csv'\n",
    "    VAL_DS_PATH = './dataset/val.csv'\n",
    "    TEST_DS_PATH = './dataset/finaltest.csv'\n",
    "    \n",
    "    # images dir\n",
    "    TRAIN_IMG_DIR = './dataset/train/'\n",
    "    VAL_IMG_DIR  = './dataset/val/'\n",
    "    TEST_IMG_DIR  = './dataset/finaltest/'\n",
    "    \n",
    "    # training hyperparams\n",
    "    EPOCHS = 25\n",
    "    LR = 5e-3\n",
    "    BATCH_SIZE = 128\n",
    "    NUM_WORKERS = 0\n",
    "    SHUFFLE = True\n",
    "    \n",
    "    # saved model path\n",
    "    MODEL_DIR = './model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a43450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a2325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard writer\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "writer = SummaryWriter(f\"runs/{current_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd927d2d",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e5d1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/omarsayed7/Deep-Emotion/blob/master/data_loaders.py\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, datatype, transform):\n",
    "        '''\n",
    "        Pytorch Dataset class\n",
    "        params:-\n",
    "                 csv_file : the path of the csv file    (train, validation, test)\n",
    "                 img_dir  : the directory of the images (train, validation, test)\n",
    "                 datatype : data type for the dataset   (train, val, test)\n",
    "                 transform: pytorch transformation over the data\n",
    "        return :-\n",
    "                 image, labels\n",
    "        '''\n",
    "        self.csv_file = pd.read_csv(csv_file)\n",
    "        self.labels = self.csv_file['emotion']\n",
    "        self.img_dir = img_dir\n",
    "        self.datatype = datatype\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "#         img = Image.open(self.img_dir + self.datatype + str(idx) + '.jpg').convert('RGB')\n",
    "        img = Image.open(self.img_dir + self.datatype + str(idx) + '.jpg')\n",
    "        labels = np.array(self.labels[idx])\n",
    "        labels = torch.from_numpy(labels).long()\n",
    "\n",
    "        if self.transform :\n",
    "            img = self.transform(img)\n",
    "        return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "499e6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranformations\n",
    "# transformation= transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "# #     transforms.Grayscale(3), # no need this since you .convert(\"RGB\") at __getitem__\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f25683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset = EmotionDataset(\n",
    "    csv_file=cfg.TRAIN_DS_PATH, \n",
    "    img_dir=cfg.TRAIN_IMG_DIR,\n",
    "    datatype='train',\n",
    "    transform=transformation\n",
    ")\n",
    "\n",
    "validation_dataset = EmotionDataset(\n",
    "    csv_file=cfg.VAL_DS_PATH, \n",
    "    img_dir=cfg.VAL_IMG_DIR,\n",
    "    datatype='val',\n",
    "    transform = transformation\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = EmotionDataset(\n",
    "    csv_file=cfg.TEST_DS_PATH, \n",
    "    img_dir=cfg.TEST_IMG_DIR,\n",
    "    datatype='finaltest',\n",
    "    transform = transformation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "460c8bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=cfg.BATCH_SIZE, \n",
    "    shuffle =cfg.SHUFFLE, \n",
    "    num_workers=cfg.NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=cfg.BATCH_SIZE, \n",
    "    shuffle =cfg.SHUFFLE, \n",
    "    num_workers=cfg.NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=cfg.BATCH_SIZE, \n",
    "    shuffle =cfg.SHUFFLE, \n",
    "    num_workers=cfg.NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2f276",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b77d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, train_loader, criterion, optimizer, device, log_every_n_batches=100, writer=None):\n",
    "    \"\"\"Train the model for 1 epoch\n",
    "    Args:\n",
    "        epoch: Current training epoch\n",
    "        model: nn.Module\n",
    "        train_loader: train DataLoader\n",
    "        criterion: callable loss function\n",
    "        optimizer: pytorch optimizer\n",
    "        device: torch.device\n",
    "        log_every_n_batches: Metrics will log every n batches\n",
    "        writer: Tensorboard SummaryWriter\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Float, Float]\n",
    "        average train loss and average train accuracy for current epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    train_losses = []\n",
    "    train_corrects = []\n",
    "    running_losses = 0.0\n",
    "    running_corrects = 0.0\n",
    "    total = 0.0\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over data.\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # prediction\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # calculate loss & number of corrects for current batch\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        n_corrects = torch.sum(preds == labels.data).item()\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate runnings_loss, running_correct and total if log_every_n_batches > 0\n",
    "        if log_every_n_batches:\n",
    "            running_losses += loss.item()\n",
    "            running_corrects += n_corrects\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            if batch_idx % log_every_n_batches == 0:\n",
    "                print(f\"loss: {loss.item():>7f}  [{batch_idx:>5d}/{len(train_loader):>5d}]\")\n",
    "                \n",
    "                # write to tensorboard\n",
    "                if writer:\n",
    "                    writer.add_scalar(\"train_loss\", \n",
    "                                      running_losses/log_every_n_batches,\n",
    "                                      global_step=epoch*len(train_loader)+batch_idx)\n",
    "                    writer.add_scalar(\"train_accuracy\", \n",
    "                                      running_corrects/total,\n",
    "                                      global_step=epoch*len(train_loader)+batch_idx)\n",
    "                \n",
    "                # reset to zero values for next logging\n",
    "                running_losses = 0.0\n",
    "                running_corrects = 0.0\n",
    "                total = 0.0\n",
    "            \n",
    "        # accumulate loss and n_corrects for current batch for later average metric calculation\n",
    "        train_losses.append(loss.item())\n",
    "        train_corrects.append(n_corrects)\n",
    "        \n",
    "    ave_train_loss = sum(train_losses)/len(train_losses)\n",
    "    ave_train_accuracy = sum(train_corrects)/len(train_loader.dataset)      \n",
    "\n",
    "    return ave_train_loss, ave_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1afae531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(epoch, model, val_loader, criterion, device, log_every_n_batches=100, writer=None):\n",
    "    \"\"\"Validate the model for 1 epoch\n",
    "    Args:\n",
    "        epoch: Current validation epoch\n",
    "        model: nn.Module\n",
    "        val_loader: val DataLoader\n",
    "        criterion: callable loss function\n",
    "        device: torch.device\n",
    "        log_every_n_batches: Metrics will log every n batches\n",
    "        writer: Tensorboard SummaryWriter\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Float, Float]\n",
    "        average val loss and average val accuracy for current epoch\n",
    "    \"\"\"\n",
    "\n",
    "    val_losses = []\n",
    "    val_corrects = []\n",
    "    running_losses = 0.0\n",
    "    running_corrects = 0.0\n",
    "    total = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over data\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # prediction\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # calculate loss & number of corrects for current batch\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            n_corrects = torch.sum(preds == labels.data).item()\n",
    "            \n",
    "            # calculate runnings_loss, running_correct and total if log_every_n_batches > 0\n",
    "            if log_every_n_batches:\n",
    "                running_losses += loss.item()\n",
    "                running_corrects += n_corrects\n",
    "                total += labels.size(0)\n",
    "\n",
    "                if batch_idx % log_every_n_batches == 0:\n",
    "                    if writer:\n",
    "                        writer.add_scalar(\"val_loss\", \n",
    "                                          running_losses/log_every_n_batches,\n",
    "                                          global_step=epoch*len(val_loader)+batch_idx)\n",
    "                        writer.add_scalar(\"val_accuracy\", \n",
    "                                          running_corrects/total,\n",
    "                                          global_step=epoch*len(val_loader)+batch_idx)\n",
    "\n",
    "                    # reset to zero values for next logging\n",
    "                    running_losses = 0.0\n",
    "                    running_corrects = 0.0\n",
    "                    total = 0.0\n",
    "                \n",
    "            # accumulate loss and n_corrects for current batch for later average metric calculation\n",
    "            val_losses.append(loss.item())\n",
    "            val_corrects.append(n_corrects)\n",
    "            \n",
    "        ave_val_loss = sum(val_losses)/len(val_losses)\n",
    "        ave_val_accuracy = sum(val_corrects)/len(val_loader.dataset)\n",
    "\n",
    "    return ave_val_loss, ave_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa817a",
   "metadata": {},
   "source": [
    "# Training and Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58bc6c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep-Emotion Model\n",
    "# reference: https://github.com/omarsayed7/Deep-Emotion/blob/master/deep_emotion.py\n",
    "class Deep_Emotion(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Deep_Emotion class contains the network architecture.\n",
    "        '''\n",
    "        super(Deep_Emotion,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,3)\n",
    "        self.conv2 = nn.Conv2d(10,10,3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(10,10,3)\n",
    "        self.conv4 = nn.Conv2d(10,10,3)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.norm = nn.BatchNorm2d(10)\n",
    "\n",
    "        self.fc1 = nn.Linear(810,50)\n",
    "        self.fc2 = nn.Linear(50,7)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(640, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 640)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size(), align_corners=True)\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x\n",
    "\n",
    "    def forward(self,input):\n",
    "        out = self.stn(input)\n",
    "\n",
    "        out = F.relu(self.conv1(out))\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(self.pool2(out))\n",
    "\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = self.norm(self.conv4(out))\n",
    "        out = F.relu(self.pool4(out))\n",
    "\n",
    "        out = F.dropout(out)\n",
    "        out = out.view(-1, 810)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "599b7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model instance\n",
    "model = Deep_Emotion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19b36628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pretrained model\n",
    "# model = models.mobilenet.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# # freeze all layers\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# num_features = model.classifier[1].in_features\n",
    "# model.classifier[1] = nn.Linear(num_features, len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe3cfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# transfer to cuda device if any\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c76f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, optimizer and scheduler \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=cfg.LR)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "# lambda1 = lambda epoch: 0.65 ** epoch\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "\n",
    "# optimizer = SGD(model.parameters(), lr=cfg.LR)\n",
    "# scheduler = lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1,step_size_up=5,mode=\"exp_range\",gamma=0.85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3a93309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ivand\\projects\\ai\\emotion-recognition\\venv\\lib\\site-packages\\torch\\nn\\functional.py:4193: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.997590  [    0/  225]\n",
      "loss: 1.628915  [  100/  225]\n",
      "loss: 1.705912  [  200/  225]\n",
      "Average TrainLoss: 1.6749 \tAverage TrainAcc: 0.3327\n",
      "Evaluation:\n",
      "Average ValLoss: 1.6121 \tAverage ValAcc: 0.3739\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.668635  [    0/  225]\n",
      "loss: 1.505772  [  100/  225]\n",
      "loss: 1.366452  [  200/  225]\n",
      "Average TrainLoss: 1.5191 \tAverage TrainAcc: 0.4091\n",
      "Evaluation:\n",
      "Average ValLoss: 1.4978 \tAverage ValAcc: 0.4302\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.435325  [    0/  225]\n",
      "loss: 1.438592  [  100/  225]\n",
      "loss: 1.459779  [  200/  225]\n",
      "Average TrainLoss: 1.4468 \tAverage TrainAcc: 0.4358\n",
      "Evaluation:\n",
      "Average ValLoss: 1.4234 \tAverage ValAcc: 0.4497\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.509807  [    0/  225]\n",
      "loss: 1.397484  [  100/  225]\n",
      "loss: 1.301111  [  200/  225]\n",
      "Average TrainLoss: 1.4046 \tAverage TrainAcc: 0.4571\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3940 \tAverage ValAcc: 0.4795\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.479603  [    0/  225]\n",
      "loss: 1.389777  [  100/  225]\n",
      "loss: 1.399631  [  200/  225]\n",
      "Average TrainLoss: 1.3859 \tAverage TrainAcc: 0.4647\n",
      "Evaluation:\n",
      "Average ValLoss: 1.4037 \tAverage ValAcc: 0.4539\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.407975  [    0/  225]\n",
      "loss: 1.312713  [  100/  225]\n",
      "loss: 1.205630  [  200/  225]\n",
      "Average TrainLoss: 1.3692 \tAverage TrainAcc: 0.4715\n",
      "Evaluation:\n",
      "Average ValLoss: 1.4140 \tAverage ValAcc: 0.4634\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.295421  [    0/  225]\n",
      "loss: 1.353066  [  100/  225]\n",
      "loss: 1.305448  [  200/  225]\n",
      "Average TrainLoss: 1.3529 \tAverage TrainAcc: 0.4786\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3364 \tAverage ValAcc: 0.4854\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.407878  [    0/  225]\n",
      "loss: 1.367234  [  100/  225]\n",
      "loss: 1.401531  [  200/  225]\n",
      "Average TrainLoss: 1.3425 \tAverage TrainAcc: 0.4815\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3562 \tAverage ValAcc: 0.4831\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.184373  [    0/  225]\n",
      "loss: 1.325832  [  100/  225]\n",
      "loss: 1.332814  [  200/  225]\n",
      "Average TrainLoss: 1.3336 \tAverage TrainAcc: 0.4866\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3847 \tAverage ValAcc: 0.4742\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.310043  [    0/  225]\n",
      "loss: 1.273522  [  100/  225]\n",
      "loss: 1.383137  [  200/  225]\n",
      "Average TrainLoss: 1.3264 \tAverage TrainAcc: 0.4907\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3815 \tAverage ValAcc: 0.4784\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.272666  [    0/  225]\n",
      "loss: 1.396765  [  100/  225]\n",
      "loss: 1.374887  [  200/  225]\n",
      "Average TrainLoss: 1.3197 \tAverage TrainAcc: 0.4922\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3713 \tAverage ValAcc: 0.4759\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.389947  [    0/  225]\n",
      "loss: 1.402524  [  100/  225]\n",
      "loss: 1.355222  [  200/  225]\n",
      "Average TrainLoss: 1.3196 \tAverage TrainAcc: 0.4918\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3484 \tAverage ValAcc: 0.4876\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.288522  [    0/  225]\n",
      "loss: 1.268329  [  100/  225]\n",
      "loss: 1.296204  [  200/  225]\n",
      "Average TrainLoss: 1.3051 \tAverage TrainAcc: 0.4966\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3482 \tAverage ValAcc: 0.4687\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.450049  [    0/  225]\n",
      "loss: 1.263855  [  100/  225]\n",
      "loss: 1.266415  [  200/  225]\n",
      "Average TrainLoss: 1.3018 \tAverage TrainAcc: 0.5019\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3711 \tAverage ValAcc: 0.4776\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.140083  [    0/  225]\n",
      "loss: 1.240299  [  100/  225]\n",
      "loss: 1.302034  [  200/  225]\n",
      "Average TrainLoss: 1.2958 \tAverage TrainAcc: 0.5012\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3867 \tAverage ValAcc: 0.4742\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.234225  [    0/  225]\n",
      "loss: 1.503568  [  100/  225]\n",
      "loss: 1.381058  [  200/  225]\n",
      "Average TrainLoss: 1.2918 \tAverage TrainAcc: 0.5049\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3568 \tAverage ValAcc: 0.4865\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.155304  [    0/  225]\n",
      "loss: 1.346683  [  100/  225]\n",
      "loss: 1.317316  [  200/  225]\n",
      "Average TrainLoss: 1.2951 \tAverage TrainAcc: 0.5028\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3560 \tAverage ValAcc: 0.4770\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.238166  [    0/  225]\n",
      "loss: 1.364414  [  100/  225]\n",
      "loss: 1.226399  [  200/  225]\n",
      "Average TrainLoss: 1.2809 \tAverage TrainAcc: 0.5056\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3074 \tAverage ValAcc: 0.4904\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.109335  [    0/  225]\n",
      "loss: 1.303663  [  100/  225]\n",
      "loss: 1.382303  [  200/  225]\n",
      "Average TrainLoss: 1.2732 \tAverage TrainAcc: 0.5107\n",
      "Evaluation:\n",
      "Average ValLoss: 1.2988 \tAverage ValAcc: 0.4904\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.212493  [    0/  225]\n",
      "loss: 1.419096  [  100/  225]\n",
      "loss: 1.241413  [  200/  225]\n",
      "Average TrainLoss: 1.2724 \tAverage TrainAcc: 0.5106\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3215 \tAverage ValAcc: 0.4798\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.205851  [    0/  225]\n",
      "loss: 1.236308  [  100/  225]\n",
      "loss: 1.258166  [  200/  225]\n",
      "Average TrainLoss: 1.2702 \tAverage TrainAcc: 0.5103\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3370 \tAverage ValAcc: 0.4974\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.064515  [    0/  225]\n",
      "loss: 1.290841  [  100/  225]\n",
      "loss: 1.229955  [  200/  225]\n",
      "Average TrainLoss: 1.2620 \tAverage TrainAcc: 0.5151\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3560 \tAverage ValAcc: 0.4848\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.215826  [    0/  225]\n",
      "loss: 1.348853  [  100/  225]\n",
      "loss: 1.200397  [  200/  225]\n",
      "Average TrainLoss: 1.2661 \tAverage TrainAcc: 0.5152\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3422 \tAverage ValAcc: 0.4926\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.323687  [    0/  225]\n",
      "loss: 1.305286  [  100/  225]\n",
      "loss: 1.244184  [  200/  225]\n",
      "Average TrainLoss: 1.2562 \tAverage TrainAcc: 0.5182\n",
      "Evaluation:\n",
      "Average ValLoss: 1.2920 \tAverage ValAcc: 0.4940\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.189589  [    0/  225]\n",
      "loss: 1.092018  [  100/  225]\n",
      "loss: 1.274305  [  200/  225]\n",
      "Average TrainLoss: 1.2567 \tAverage TrainAcc: 0.5176\n",
      "Evaluation:\n",
      "Average ValLoss: 1.3345 \tAverage ValAcc: 0.4898\n",
      "\n",
      "\n",
      "Training complete in 6m 56s\n"
     ]
    }
   ],
   "source": [
    "# Start training and val loops\n",
    "\n",
    "since = time.time()\n",
    "for epoch in range(cfg.EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    \n",
    "    # Train\n",
    "    ave_train_loss, ave_train_acc = train_epoch(epoch, model, train_loader, criterion, optimizer, device, 100, writer)\n",
    "    print(f'Average TrainLoss: {ave_train_loss:.4f} \\tAverage TrainAcc: {ave_train_acc:.4f}')\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"Evaluation:\")\n",
    "    ave_val_loss, ave_val_acc = val_epoch(epoch, model, val_loader, criterion, device, 100, writer)\n",
    "    print(f'Average ValLoss: {ave_val_loss:.4f} \\tAverage ValAcc: {ave_val_acc:.4f}')\n",
    "    \n",
    "    # log metrics at every epoch\n",
    "    if writer:\n",
    "        writer.add_scalar(\"epoch_train_loss\", ave_train_loss, epoch)\n",
    "        writer.add_scalar(\"epoch_train_accuracy\", ave_train_acc, epoch)\n",
    "        writer.add_scalar(\"epoch_val_loss\", ave_val_loss, epoch)\n",
    "        writer.add_scalar(\"epoch_val_accuracy\", ave_val_acc, epoch)\n",
    "    \n",
    "    # schedule lr\n",
    "    scheduler.step(ave_val_loss)\n",
    "#     scheduler.step()\n",
    "    print(\"\\n\")\n",
    "        \n",
    "time_elapsed = time.time() - since\n",
    "print(f\"Training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be149f",
   "metadata": {},
   "source": [
    "# Evaluating on Unseen Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "432117e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average TestLoss: 1.3491 \tAverage TestAcc: 0.4806\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_corrects = []\n",
    "\n",
    "model.eval()\n",
    "# Iterate over data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # prediction\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # calculate loss\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # statistics\n",
    "        test_losses.append(loss.item())\n",
    "        test_corrects.append(torch.sum(preds == labels.data).item())\n",
    "\n",
    "    ave_test_loss = sum(test_losses)/len(test_losses)\n",
    "    ave_test_acc = sum(test_corrects)/len(test_loader.dataset)\n",
    "    \n",
    "print(f'Average TestLoss: {ave_test_loss:.4f} \\tAverage TestAcc: {ave_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f608082",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa15c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "model_name = f\"model_de_{current_time}_epoch{cfg.EPOCHS}_lr{cfg.LR}_batch{cfg.BATCH_SIZE}_acc{ave_test_acc:.3f}.pt\"\n",
    "torch.save(model.state_dict(), cfg.MODEL_DIR + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2785c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aff473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
